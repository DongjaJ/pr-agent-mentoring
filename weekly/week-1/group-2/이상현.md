# 1주차 개인 학습 정리

---

# LLM - Transformer

## 🔶 Encoder

Transformer의 Encoder 블록은 총 두 개의 주요 레이어로 구성되어 있다:

1. **Self-Attention Layer**
2. **Feed Forward Layer**

이 외에도, **Multi-Head Attention**과 **Positional Encoding**,

그리고 **Residual Connection + Layer Normalization** 같은 보조 구조가 함께 동작한다.

---

### 🔷 Self-Attention Layer

입력 문장 내 **다른 위치에 있는 단어들과의 관계**를 고려해

현재 단어를 더 잘 인코딩할 수 있도록 하는 구조다.

즉, 단어와 문맥을 연결하는 핵심 과정이다.

### Self-Attention 동작 과정

1. **Query, Key, Value 벡터 생성**
    
    각 단어 임베딩에 학습된 가중치 행렬을 곱해 **Query, Key, Value 벡터**를 생성한다.
    
    - Query: 지금 단어가 **어떤 문맥을 찾고 싶은지**
    - Key: 각 단어가 **어떤 문맥 특성을 갖는지**
    - Value: 해당 단어가 **실제로 갖고 있는 정보**
2. **유사도 점수 계산**
    
    현재 단어의 Query와 모든 단어들의 Key를 내적해 **어느 단어와 가장 관련 있는지** 점수로 나타낸다.
    
3. **스케일링**
    
    점수가 너무 커지는 걸 방지하기 위해, Key 벡터 차원의 제곱근으로 나눈다.
    
    → 그래야 softmax 적용 시 출력이 0이나 1에 치우치지 않고,
    
    → **기울기가 사라지지 않아서 안정적인 학습**이 가능해진다.
    
4. **Softmax 정규화**
    
    점수를 확률처럼 정규화해,
    
    각 단어가 얼마나 집중 대상이 되는지 결정한다.
    
5. **Value 가중합 계산**
    
    각 Value 벡터에 softmax 점수를 곱한 뒤, 관련성이 높은 단어의 정보를 더 크게 반영한다.
    
6. **출력 생성**
    
    모든 Value 벡터의 가중합을 통해
    
    현재 단어에 대한 Self-Attention 출력 벡터를 얻는다.
    
    이 결과는 다음 레이어인 Feed Forward로 전달된다.
    

---

### 🔷 Multi-Head Attention

Self-Attention을 **여러 개의 시각으로 동시에** 수행하는 구조다.

Transformer에서는 보통 **8개의 attention head**를 사용한다.

### Multi-Head Attention의 이점

1. **자기 자신에 집중되는 현상 보완**
    
    단일 attention에서는 종종 단어가 자기 자신에만 집중하게 되는데,
    
    여러 head를 사용하면 **다양한 문맥 요소에 분산해서 주의**를 줄 수 있다.
    
2. **다양한 표현 공간 제공**
    
    각 head마다 **다른 Q/K/V 가중치 세트**를 사용해,
    
    동일한 단어도 **서로 다른 관점의 문맥 표현**을 학습하게 된다.
    

### 구조 흐름

- Self-Attention 계산을 **8번 독립적으로 수행**
- 각 head의 출력을 **concatenate**한 뒤
- 출력 가중치 행렬을 곱해 **최종 하나의 벡터**로 만든다

이 과정을 통해 **하나의 단어 표현 안에 다양한 문맥 요소**를 담을 수 있게 된다.

---

### 🔷 Positional Encoding

Transformer는 **입력 순서를 직접 다루지 않기 때문에**,

입력 단어의 **순서 정보를 인코딩**해서 임베딩에 더해줘야 한다.

### 위치 정보를 더하는 이유

- Transformer는 RNN처럼 순차적으로 입력을 보지 않는다
- 따라서 “내가 몇 번째 단어인지”를 알려주는 신호가 필요하다

### 구성 방식

- sin과 cos 함수를 기반으로 한 **수학적 패턴**을 따르며,
- 이 벡터는 각 단어 임베딩에 더해져
    
    **단어 간 상대적 거리나 위치 정보를 유지**할 수 있게 해준다
    

---

### 🔷 Feed Forward Layer

Self-Attention으로 문맥을 파악한 후,

각 단어의 벡터를 **독립적으로 가공하는 레이어**다.

두 개의 선형 변환과 비선형 활성화를 포함한 작은 MLP 구조로 되어 있다.

### 역할 및 구성

- **Self-Attention의 선형적 처리** 이후,
    
    **비선형성**을 추가해서 **표현력을 높인다**
    
- 각 단어마다 **독립적으로 적용**되며
    
    입력 → ReLU → 출력의 구조를 따른다
    

---

### 🔷 Residual Connection & Layer Normalization

Transformer의 모든 주요 서브 레이어(Self-Attention, Feed Forward)는

다음과 같은 구조를 따른다:

### 각각의 역할

| 구성 요소 | 설명 |
| --- | --- |
| **Residual Connection** | 입력 정보를 보존하여 **정보 손실을 줄이고** 역전파도 원활하게 함 |
| **Layer Normalization** | 출력의 분포를 정규화하여 **학습을 안정적으로 유지**함 |

→ 이 두 구조는 **Transformer의 깊은 네트워크가 잘 학습되도록 돕는 핵심 장치**다.

---

## 🔶 Decoder

Transformer의 디코더는 인코더의 출력과 이전 시점의 출력을 활용해 **다음 단어를 생성**하는 역할을 한다.

각 디코더 블록은 다음과 같은 세 가지 주요 서브 레이어로 구성된다:

- **Masked Self-Attention Layer**
- **Encoder-Decoder Attention Layer**
- **Feed Forward Layer**

그리고 인코더와 마찬가지로 **각 레이어는 Residual Connection + Layer Normalization**으로 감싸져 있다.

---

### 🔷 Self-Attention Layer (Masked)

디코더의 Self-Attention은 인코더와는 **약간 다르게** 작동한다.

**미래 단어를 보지 못하도록 마스킹**을 적용해 **오직 이전 단어까지만 참조 가능**하게 만든다.

### 마스킹

- 출력 시퀀스를 생성할 때 **미래 단어를 참조하는 건 허용되지 않으므로,**
    
    softmax 계산 전에 **미래 위치를 -∞로 설정하여 마스킹**한다.
    
- 이로 인해 해당 위치의 attention 값은 0이 되며, **현재 시점 이전의 단어들만 보고 결정**하게 된다.

이 구조는 **순차적인 단어 생성을 가능하게** 하며,

**문장 생성 시 언어의 흐름을 자연스럽게 학습**할 수 있도록 돕는다.

---

### 🔷 Encoder-Decoder Attention

이 레이어는 디코더에서만 존재하는 레이어로,

**인코더의 출력**을 참조하여 디코더가 **입력 문장의 특정 부분에 집중**할 수 있게 한다.

- **Query**는 디코더의 아래 레이어에서 가져오고,
- **Key / Value**는 인코더의 최종 출력에서 가져온다.

디코더는 자신이 만든 **Query**로, 인코더가 만든 입력 문장의 **Key / Value**와 비교해

**어디를 참조해야 할지를 계산**하는 구조다.

---

### 🔷 Feed Forward Layer

Self-Attention과 Encoder-Decoder Attention을 거친 후,

각 단어의 표현은 **Feed Forward Layer**를 통해 **비선형 가공**된다.

- 두 개의 선형 변환 + ReLU 활성화 함수로 구성
- 각 단어 위치에 대해 **독립적으로 동일한 FFN을 적용**

이후 **Residual Connection + Layer Normalization**을 통해 정보 손실 없이 다음 블록으로 전달된다.

---

## 🔷 Output: Final Linear + Softmax Layer

디코더의 마지막 출력은 **벡터 형태**로 나오기 때문에,

이를 실제 단어로 바꾸기 위해 두 개의 레이어가 추가로 사용된다.

1. **Linear Layer**
    - 디코더 출력을 **출력 단어 수만큼의 차원**을 가진 **logits 벡터**로 변환
2. **Softmax Layer**
    - logits 벡터를 확률 분포로 변환
    - 가장 높은 확률을 갖는 단어를 선택 → **해당 시점의 출력 단어로 사용**

---

## 🔷 Training 과정

Transformer의 학습은 **훈련용 정답 문장**과 **모델의 출력**을 비교하며 진행된다.

### 예시: "merci" → "thanks"

- 학습 초기, 모델의 가중치는 무작위이므로 **출력은 엉뚱한 확률 분포**를 만든다.
- 이 확률 분포와 실제 정답 단어의 **one-hot 벡터**를 비교하여 오차를 계산하고,
- 그 오차를 바탕으로 역전파를 통해 가중치를 조정한다.

### 손실 함수(Loss Function)

- **Cross-Entropy Loss**를 사용해 **예측 분포와 실제 분포의 차이**를 계산

---

## 🔷 번역 결과 생성 방식

Transformer는 한 번에 하나의 단어씩 예측하며 문장을 완성한다.

이때 출력 단어를 고르는 방법은 크게 두 가지다:

### 1. **Greedy Decoding**

- 각 스텝에서 **가장 확률이 높은 단어 하나**만 선택
- 단순하지만 전역 최적이 아닐 수 있음

### 2. **Beam Search**

- **여러 후보를 유지하며 탐색**
- 예: beam size = 2이면 매 시점마다 **가장 가능성 높은 2개의 시나리오**를 유지하고 확장
- 정확도는 높지만 속도는 느려짐
- beam size와 top_k는 **튜닝 가능한 하이퍼파라미터**

참고자료 : https://nlpinkorean.github.io/illustrated-transformer/