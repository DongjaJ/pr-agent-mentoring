## 학습

[대형 언어 모델(LLM)의 심층 분석 : ChatGPT의 작동 방식 이해하기](https://www.youtube.com/watch?v=6PTCwRRUHjE) 영상 시청 및 학습

ai를 이용한 유튜브 영상 요약 + 영상 시청으로 학습 중

## 요약

### ⚙️ LLM 구축 과정: 사전 훈련 단계

LLM 구축의 첫 번째 단계는 사전 훈련입니다. 이 단계에서는 인터넷에서 데이터를 다운로드하여 처리합니다. 허깅 페이스의 Fine Web 데이터 세트와 유사한 것을 사용하며, OpenAI, Anthropic, Google 등의 주요 LLM 제공업체도 유사한 데이터 세트를 가지고 있을 것입니다.

사전 훈련의 목표는 공개적으로 사용 가능한 소스에서 인터넷의 방대한 텍스트를 수집하는 것입니다. 고품질 문서와 다양한 지식을 확보하는 것이 중요하며, 문서의 양도 매우 많아야 합니다. 프로덕션 환경에서는 약 44테라바이트의 디스크 공간이 필요합니다.

Common Crawl의 데이터가 이 과정의 출발점이며, 2007년부터 인터넷을 샅샅이 훑어온 단체입니다. 수집된 데이터는 URL 필터링(유해 사이트, 스팸 등등), 텍스트 추출, 언어 필터링, 중복 제거, PII(개인정보) 제거 등 다양한 방식으로 필터링됩니다. FineWeb 데이터 세트는 전처리 과정의 결과물입니다. (손실 압축)

#### 📝 텍스트 표현: 토큰화

신경망에 텍스트를 입력하기 위해서는 먼저 컴퓨터가 이해할 수 있는 형태로 변환해야 합니다. 신경망은 기본적으로 유한한 기호 집합으로 구성된 1차원 시퀀스를 처리합니다. 이러한 변환 과정을 '토큰화'라고 합니다.

#### 토큰화 과정

1. 텍스트 인코딩: 먼저 텍스트를 UTF-8과 같은 인코딩 방식을 사용하여 이진 데이터(0과 1)로 변환합니다.
2. 바이트 표현: 이진 데이터를 8비트 단위(바이트)로 그룹화하여 0-255 사이의 값으로 표현합니다. 이를 통해 256가지 기본 기호를 표현할 수 있습니다.
3. [바이트 쌍 인코딩](https://wikidocs.net/22592)(BPE): 텍스트에서 자주 함께 등장하는 연속된 바이트나 기호들을 분석하여 하나의 새로운 토큰으로 병합합니다. 이 과정을 반복하여 효율적인 어휘 사전을 구축합니다.
4. 사전 구축: 이렇게 생성된 토큰들을 모아 모델의 어휘 사전을 구성합니다.

#### 토큰화의 중요성

토큰화 과정에서는 어휘 사전의 크기(토큰 수)와 시퀀스 길이 사이의 균형이 중요합니다. 사전이 너무 작으면 각 토큰이 담는 정보량이 적어 시퀀스가 길어지고, 너무 크면 메모리 사용량이 증가하고 학습이 어려워집니다.

예를 들어, GPT-4는 약 100,277개의 토큰을 포함하는 어휘 사전을 사용합니다. 이 토큰들은 단일 문자부터 자주 사용되는 단어, 단어 조각까지 다양합니다.

텍스트가 실제로 어떻게 토큰화되는지 확인하려면 [Tick Tokenizer](https://tiktokenizer.vercel.app/)와 같은 도구를 활용할 수 있습니다. 이를 통해 특정 텍스트가 GPT-4와 같은 모델에 어떻게 입력되는지 시각화할 수 있습니다.

### 🧠 신경망 훈련: 통계적 관계 모델링

신경망 훈련 단계에서는 토큰들이 시퀀스에서 서로 어떻게 따라가는지에 대한 통계적 관계를 모델링합니다. 데이터에서 토큰 창을 만들고, 시퀀스에서 다음에 오는 토큰을 예측합니다. 신경망은 다음에 올 토큰의 확률을 출력하며, 훈련 초기에는 무작위로 초기화됩니다.

신경망은 다음에 오는 올바른 토큰에 더 높은 확률을 부여하기 위해 업데이트됩니다. 이 과정은 전체 데이터 세트의 모든 토큰에 대해 동시에 발생하며, 이것이 신경망을 훈련하는 과정입니다. 훈련 세트에서 실제로 발생하는 통계와 예측이 일치하도록 업데이트합니다.

신경망 내부에는 입력 토큰이 있으며, 이는 파라미터 또는 가중치와 함께 거대한 수학적 표현식으로 혼합됩니다. 처음에는 이러한 파라미터가 완전히 무작위로 설정되지만, 반복적인 업데이트를 통해 훈련 세트에서 보이는 패턴과 일관성을 갖게 됩니다.

### ✨ 추론 단계: 새로운 데이터 생성

추론 단계에서는 모델에서 새로운 데이터를 생성합니다. 원하는 시작점과 같은 접두사인 일부 토큰으로 시작하여 네트워크에 입력하면 네트워크가 확률을 제공합니다. 이 확률 분포에 따라 토큰을 샘플링하여 모델에서 생성합니다.

각 단계마다 동전을 던지듯 샘플링하며, 운이 좋으면 훈련 세트에서 텍스트의 작은 부분을 재현할 수 있지만, 훈련 데이터의 어떤 문서에도 그대로 포함되지 않은 토큰을 얻을 수도 있습니다. 훈련에서 본 데이터의 리믹스 같은 것을 얻게 될 것입니다.

가장 일반적인 시나리오에서 인터넷을 다운로드하고 토큰화하는 것은 전처리 단계입니다. 네트워크 훈련을 수행하고 만족스러운 특정 매개변수 집합을 얻으면 모델을 가져와서 추론을 수행하고 실제로 모델에서 데이터를 생성할 수 있습니다. Chat GPT에서 모델과 대화할 때, 해당 모델은 추론만 수행합니다.

### 💰 연산: GPU 골드 러시

신경망 훈련에는 많은 연산이 필요하며, GPU가 이러한 연산에 완벽하게 적합합니다. GPU는 계산 비용이 많이 들지만 계산에서 많은 병렬성을 나타내기 때문입니다. 현재 모든 빅테크 기업들이 GPU를 확보하기 위해 경쟁하고 있습니다.

일론 머스크가 단일 데이터 센터에 10만 개의 GPU를 확보하는 것이 중요한 이유는, 이 모든 GPU가 시퀀스에서 다음 토큰을 예측하고 네트워크를 개선하려고 노력하기 때문입니다. GPU가 많을수록 데이터 세트를 더 빠르게 처리하고 더 큰 네트워크를 구축하고 훈련할 수 있습니다.

### 📚 기본 모델: 토큰 시뮬레이터

기본 모델은 인터넷 텍스트 토큰 시뮬레이터입니다. 그 자체로는 아직 유용하지 않지만, 시퀀스에서 다음 토큰을 예측하는 작업에서 모델이 세상에 대해 많은 것을 배웠고 그 모든 지식을 네트워크 파라미터에 저장했기 때문에 여전히 매우 유용합니다.

기본 모델은 인터넷의 일종의 압축이라고 생각할 수 있습니다. 4,050억 개의 파라미터는 일종의 zip 파일과 같지만, 무손실 압축이 아니라 손실 압축입니다. 인터넷의 게슈탈트가 남아 그것으로부터 생성할 수 있는 것과 같습니다.

기본 모델에 적절한 프롬프트를 제공하여 이 지식의 일부를 이끌어낼 수 있습니다. 예를 들어, 파리에서 볼 수 있는 최고의 랜드마크 상위 10개 목록과 같은 프롬프트를 사용할 수 있습니다. 모델은 인터넷 문서의 일부를 회상하여 결과를 내놓습니다.

### 👩‍🏫 후속 훈련: LLM을 비서로 전환

후속 훈련 단계에서는 기본 모델, 즉 인터넷 문서 시뮬레이터를 가져다가 후속 훈련에 넘깁니다. 이 단계는 계산적으로 훨씬 저렴하며, 이 LLM 모델을 비서로 전환합니다. 모델이 인터넷 문서를 샘플링하지 않고 질문에 대한 답변을 제공하도록 하려면 어떻게 해야 할까요?

우리는 대화에 대해 생각하기 시작해야 합니다. 대화는 다중 턴이 될 수 있으며, 가장 간단한 경우 인간과 비서 간의 대화입니다. 비서가 어떻게 응답해야 하는지 생각하고 이러한 대화에서 비서와 그 행동을 프로그래밍해야 합니다.

신경망이기 때문에 코드에서 명시적으로 프로그래밍하지 않고, 대화 데이터 세트에 대한 신경망 학습을 통해 어시스턴트를 암묵적으로 프로그래밍하게 됩니다. 사람 라벨러에게 대화 맥락을 제공하고, 이 상황에서 이상적인 어시스턴트 응답을 제공하도록 요청합니다.

기본 모델을 가져와서 인터넷 문서 데이터 세트를 버리고 새로운 데이터 세트로 대체합니다. 대화 데이터 세트에 대해 모델을 계속 훈련하면 모델이 매우 빠르게 조정되고 어시스턴트가 인간 쿼리에 응답하는 방식에 대한 통계를 학습합니다.

### 💬 대화의 토큰화

대화는 토큰 시퀀스로 변환되어야 합니다. 대화와 같은 데이터 구조가 토큰으로 인코딩 및 디코딩되는 방식에 대한 몇 가지 규칙이 필요합니다. 모든 LLM은 약간씩 다른 형식이나 프로토콜을 가지고 있습니다.

GPT-4o는 `I Amore start`라는 특수 토큰을 추가하여 사용자-어시스턴트 간의 차례를 구분합니다. 이러한 특수 토큰은 텍스트와 함께 삽입되어 모델이 차례의 시작과 끝을 학습하도록 합니다.

추론 시에는 컨텍스트를 구성하고 모델에서 샘플링을 시작합니다. 모델로 이동하여 시퀀스에 적합한 토큰을 묻고, LM이 역할을 하여 응답을 생성합니다. 이러한 종류의 대화가 데이터 세트에 있는 경우 이와 비슷한 느낌을 줄 것입니다.

### 🤝 인간 라벨러와 라벨링 지침

OpenAI와 같은 회사는 Upwork나 Scale AI를 통해 고용한 계약직 직원들이 대화를 구성하도록 합니다. 이들은 프롬프트를 만들고 이상적인 어시스턴트 응답을 완성하도록 요청받습니다.

OpenAI와 같이 언어 모델을 개발하는 회사는 인간이 이상적인 응답을 만드는 방법에 대한 라벨링 지침을 작성합니다. 라벨링 지침은 일반적으로 짧지 않고 수백 페이지에 달하며, 사람들은 전문적으로 연구해야 합니다.

InstructGPT 데이터 세트는 OpenAI에서 실제로 공개하지 않았지만, 우리는 이러한 설정을 따르고 자체 데이터를 수집하려는 오픈 소스 복제본을 가지고 있습니다. 예를 들어, 과거의 Open Assistant의 노력이 있습니다.

모델이 테스트 시간에 접하게 될 수 있는 모든 가능한 질문을 완전히 다룰 수는 없지만, 이러한 예제 데이터 세트가 몇 개 있다면 훈련 중에 모델은 유용하고 진실하며 무해한 어시스턴트라는 페르소나를 갖기 시작할 것입니다. 시스템은 통계적으로 이 회사가 만드는 라벨링 지침에 반영된 유용하고, 진실하며, 무해한 어시스턴트라는 페르소나를 채택합니다.

최근에는 인간이 더 이상 혼자서 모든 힘든 일을 하는 경우는 흔하지 않습니다. 기존 LLM을 사용하여 기본적으로 답변을 제시한 다음 편집하거나 하는 경우가 훨씬 더 많습니다. LLM은 기본적으로 이러한 대규모 대화 데이터 세트를 만드는 데 광범위하게 사용됩니다.

Chat GPT에 가서 질문을 하고 엔터를 누르면, 나오는 결과는 훈련 세트에서 일어나는 일과 통계적으로 일치하는 경향이 있습니다. 따라서 Chat GPT에서 실제로 무엇과 대화하고 있는 걸까요? 그것은 어떤 마법 같은 AI에서 나오는 것이 아니라, 대략적으로 말하면 회사에서 작성한 라벨링 지침에서 비롯된 인간 라벨러를 통계적으로 모방하는 것에서 나옵니다.

평균적인 라벨러와 대화하고 있는 것입니다. 이 평균적인 라벨러는 꽤 숙련되어 있을 가능성이 높지만, 이러한 데이터 세트 구성에 고용될 만한 사람의 순간적인 시뮬레이션과 대화하고 있는 것입니다.

### 🤔 LLM 심리학: 환각과 완화

LLM이 내용을 지어내거나 정보를 완전히 날조하는 경우를 환각이라고 합니다. 이 문제는 수년 전 초기 모델에 널리 존재했던 문제였고, 몇 가지 완화 방법 덕분에 조금 나아졌다고 생각합니다.

테스트 시간에 '누구는 누구인가'라고 물어볼 때, 어시스턴트가 '모르겠어요'라고 말하지 않는다는 것이 문제입니다. 모델은 통계적으로 훈련 세트를 모방하기 때문에, 답변 스타일을 취하고 최선을 다할 것입니다. 모델들은 인터넷에 접속할 수 없기 때문에, 그저 시퀀스에서 다음 토큰을 샘플링하려고 노력하고 기본적으로 내용을 지어낼 것입니다.

환각 문제를 해결하기 위해서는 데이터 세트에 모델이 특정 사실에 대해 알지 못한다는 올바른 답변이 있는 예가 필요합니다. 모델이 무엇을 알고 무엇을 모르는지, 지식의 경계를 파악하기 위해 모델을 심문하는 절차가 필요합니다. 그런 다음 모델이 모르는 것에 대해 정답이 '모델이 모른다'인 예제를 훈련 세트에 추가합니다.

메타는 훈련 세트에서 임의의 문서를 가져와서 단락을 가져온 다음 LLM을 사용하여 해당 단락에 대한 질문을 구성합니다. 모델이 이 답변에 대해 알고 있을까요? 모델에서 답변을 가져와서 정답과 비교합니다. 모델이 모르면 모델이 이 질문을 모른다는 것을 알 수 있습니다.

환각 현상을 완화하기 위해 '모른다'라고 말하는 대신 LLM에게 사실을 말하고 질문에 답할 수 있는 기회를 제공하기 위해 두 번째 완화책을 도입할 수 있습니다. 모델이 기억이나 회상을 새로 고칠 수 있게 하는 것과 동등한 것이 필요하며, 모델에 도구를 도입하여 이를 수행할 수 있습니다.

언어 모델이 특수 토큰을 내보낼 수 있는 메커니즘을 만들 수 있습니다. 모델이 모르는 질문에 답하는 대신 모델은 이제 특수 토큰 검색 시작을 내보낼 수 있는 옵션이 있으며, 이는 OpenAI의 경우 bing.com 또는 Google 검색 등으로 이동하는 쿼리입니다. 웹 검색은 도구 중 하나일 뿐입니다.

모델이 검색 시점을 결정하고, 도구를 사용하는 방법은 환각 현상과 사실성을 완화하는 추가적인 방법입니다. 신경망 파라미터에 저장된 지식은 희미한 기억이며, 컨텍스트 창을 구성하는 토큰에 담긴 지식은 작업 기억입니다.

특정 내용을 기억하도록 하려면, 그냥 직접 제공하는 것이 항상 더 효과적입니다. 모델에서 직접 사용할 수 있기 때문입니다. '당신은 누구인가요? 누가 당신을 만들었나요?' 등과 같이 묻는 것은 실제로 의미가 없습니다.

## 추후 학습에 도움될 링크

open ai에서 제공하는 프롬프팅 가이드 : https://academy.openai.com
구글에서 제공하는 프롬프팅 가이드: https://services.google.com/fh/files/misc/gemini-for-google-workspace-prompting-guide-101.pdf?utm_source=ABLEARN&utm_campaign=503cfb68d8-EMAIL_CAMPAIGN_2023_03_14_09_47_COPY_01&utm_medium=email&utm_term=0_-52908238c9-%5BLIST_EMAIL_ID%5D

## 이후 학습 예정 파트

영상 뒷 부분

### 🧠 모델은 생각하기 위해 토큰이 필요하다

모델은 정확히 같은 이유로 계산에 능숙하지 않습니다. 단일 개별 토큰에 너무 많은 것을 요구하고 있습니다. 모델이 셀 수 없고, 암산을 할 수 없다는 것을 알지만, 모델이 실제로 복사-붙여넣기를 꽤 잘한다는 것을 알고 있습니다.

모델은 글자를 보는 것이 아니라 토큰을 보기 때문에 철자와 관련된 모든 종류의 작업에 능숙하지 않습니다. 모델의 세계는 작은 텍스트 덩어리인 토큰에 관한 모든 것입니다. 모델은 우리 눈처럼 글자를 보지 못하므로 매우 간단한 글자 수준의 작업도 종종 실패합니다.

모델은 9.11이 9.9보다 크다고 하고 나름대로 정당화하지만, 당연히... 결국에는 결정을 번복하네요. 모델이 거의 성경 구절 표시처럼 보인다는 것을 인지하고, 성경 구절에서는 9.11이 9.9 다음에 나온다고 생각하는 것 같습니다.

### 💪 강화 학습: 시행착오 학습

강화 학습 단계에서는 프롬프트를 가져와서 모델을 실행합니다. 모델이 솔루션을 생성하면 솔루션을 검사합니다. 모델이 올바른 답을 맞히는 해결책을 장려합니다. 모델은 놀이터에서 노는 것과 같아서, 무엇을 얻으려고 하는지 알고, 자신에게 맞는 시퀀스를 발견하는 것입니다.

SFT 모델, 즉 지도 학습 미세 조정 모델은 모델을 올바른 해결책의 근처로 초기화하는 역할을 합니다. 모델이 해결책을 작성하거나, 방정식 시스템을 설정하는 것을 이해하거나, 해결책에 대해 이야기하는 것과 같습니다. 강화 학습은 모든 것이 맞춰지는 곳입니다.

DeepSeek R1 논문을 통해 실제로 언어 모델에 RL을 올바르게 적용했을 때 어떤 일이 발생하는지, 그리고 그 결과가 무엇인지 간략하게 살펴보겠습니다. 모델이 배우는 것은, 단계를 재평가하고 있다는 것입니다. 다양한 아이디어를 시도하고, 다양한 관점에서 시도하고, 되짚어보고, 재구성하고, 되돌아가는 것이 정확도 향상에 더 효과적이라는 것을 배웠습니다.

강화 학습을 수행할 때 인간이 게임을 하는 방식에서 벗어나는 것을 막을 수 있는 것은 아무것도 없습니다. AlphaGo는 강화 학습 과정에서 인간에게 알려지지 않았지만 돌이켜보면 훌륭한 플레이 전략을 발견했습니다.

### 🎭 검증할 수 없는 영역에서의 학습

검증할 수 없는 영역에서는 문제에 대한 다양한 솔루션의 점수를 매기기가 더 어려워집니다. 예를 들어 펠리컨에 대한 농담을 쓰는 경우 다양한 농담을 많이 생성할 수 있지만, 어떻게 점수를 매기느냐는 것입니다.

이러한 영역에서는 문제에 대한 다양한 솔루션의 점수를 매기기가 더 어려워집니다. 인간 피드백을 통한 강화 학습(RLHF)은 인간에게 보상 점수를 매기도록 요청하고, 신경망을 사용하여 인간 점수를 모방할 것입니다.

RLHF의 핵심 트릭은 간접 방식과 같습니다. 인간을 아주 조금만 관여시킬 것이고, 우리가 속이는 방법은 기본적으로 보상 모델이라고 부르는 완전히 별개의 신경망을 훈련시키는 것입니다. 그리고 이 신경망은 인간 점수를 모방할 것입니다.

하지만 중요한 것은 농담을 평가하기 위해 인간에게 10억 번이나 요청하지 않는다는 것입니다. 보상 모델이 기본적으로 점수를 제공하고, 인간의 순서와 일치하도록 훈련시키는 방법이 있다는 것을 이해하셨으면 합니다.

RLHF의 장점은 RL을 실행할 수 있게 해주고, 경험적으로 더 나은 모델을 만들 수 있게 해주며, 이상적인 응답을 쓰는 어려운 작업을 하지 않고도 사람들이 감독에 참여할 수 있게 해준다는 것입니다. 불행히도 RLHF에는 중요한 단점도 있습니다. 기본적으로 인간과 실제 인간의 판단이 아닌 인간의 손실 시뮬레이션을 통해 강화 학습을 하고 있다는 것입니다.

보상 모델을 너무 많이 실행하면 최적화가 모델을 속이기 시작하므로 기본적으로 중단하고 종료하는 것입니다. RHF는 RL이 맞지만 마법 같은 의미의 RL은 아니라는 뜻입니다.

### 🔮 LLM의 미래

모델이 매우 빠르게 멀티 모달이 될 것이라는 점입니다. 텍스트를 처리할 수 있을 뿐만 아니라 오디오를 통해 기본적으로 매우 쉽게 작동할 수 있는 LLM이 나올 것입니다. 따라서 듣고 말할 수 있으며 이미지도 볼 수 있고 그릴 수도 있습니다.

사람들이 매우 관심을 갖는 것은 모델에 개별 작업을 마치 은쟁반에 담아 건네주는 것처럼, '이 작업을 해결해 주세요'와 같이 제공된다는 점입니다. 모델은 이러한 작은 작업을 수행하지만, 작업을 수행하기 위한 일관성 있는 실행을 구성하는 것은 여전히 우리의 몫입니다.

모델이 액세스할 수 있는 유일한 유형의 학습 또는 테스트 시간 학습은 T 시간에 수행하는 작업에 따라 동적으로 조정 가능한 컨텍스트 창과 같은 인 컨텍스트 학습입니다. 하지만 저는 이것이 실제로 사람들이 하고 있는 일에 따라 실제로 학습할 수 있는 인간과는 여전히 다르다고 생각합니다.

### 📰 LLM 최신 소식 및 모델 정보

El Marina는 LLM 리더보드인데, 모든 상위 모델의 순위를 매기고 순위는 사람들의 비교를 기반으로 합니다. AI News는 Swix와 친구들이 제작하는 매우 훌륭한 뉴스레터입니다. 많은 AI 관련 소식이 X에서 나오므로 좋아하는 사람, 신뢰하는 사람을 팔로우하고 최신 정보를 X에서도 얻으세요.

가장 큰 독점 모델의 경우 해당 LLM 제공업체의 웹사이트로 가야 합니다. 오픈 웨이트 모델의 경우 LLM의 추론 제공업체로 가야 합니다. LM Studio를 사용하여 더 작은 모델들을 가져와서 로컬에서 실행할 수도 있습니다.

이 비디오가 LLM이 어떻게 훈련되고, 무엇이 되돌아오는지에 대한 몇 가지 숨겨진 세부 사항에 대한 이해를 제공했기를 바랍니다. OpenAI에서 라벨링 지침에 따라 인간 데이터 라벨러의 신경망을 시뮬레이션하는 것입니다. 그것이 우리가 지금 되돌려 받는 것입니다.
